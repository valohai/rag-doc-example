- step:
    name: create-embeddings
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    command: uv run rag-doctor create-database {parameters}
    parameters:
      - name: content_column_index
        description: Index of the document content column in the CSV files
        type: integer
        default: 0
      - name: source_column_index
        description: Index of the source link column in the CSV files
        type: integer
        default: 1
      - name: header_row_skip
        description: Number of initial rows to skip in the CSV files i.e. the header rows
        type: integer
        default: 0   
    inputs:
      - name: documentation_csv
        description: CSV file(s) containing the technical documentation
        default: https://valohai-examples.s3.amazonaws.com/rag-doc/huggingface-docs.csv

- step:
    name: do-query
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    command: uv run rag-doctor query {parameters}
    parameters:
      - name: question
        description: The question(s) to ask about the documentation
        type: string
        multiple: repeat
      - name: provider                   
        description: LLM provider to use (openai or anthropic)
        type: string
        default: openai
        choices: [openai, anthropic]      
    inputs:
      - name: embedding_db
        description: Zip archive containing the Qdrant vector database for the embeddings

- step:
    name: evaluate-rag
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    command: uv run rag-doctor evaluate {parameters}
    inputs:
      - name: responses
        description: JSON file(s) containing generated responses to evaluate
      - name: gold_standards
        description: CSV file containing gold standard retrieval indices for evaluation
        default: datum://019ab653-a0ee-17b5-3401-99e618e2c81e

- endpoint:
    name: ask
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    port: 8000
    server-command: uv run --group server rag-doctor serve --host 0.0.0.0 --database_dir ./qdrant_data --provider openai
    files:
      - name: embedding_db
        path: qdrant_data/embeddings.zip
        description: Zip archive containing the Qdrant vector database for the embeddings

- pipeline:
    name: assistant-pipeline
    nodes:
      - name: embeddings
        type: execution
        step: create-embeddings
      - name: manual-evaluation
        type: execution
        step: do-query
        override:
          parameters:
            - name: question
              default: ["who is Michael Jordan?", "what is a dataset?", "hello", "how to create a model?"]
    edges:
      - [embeddings.output.*, manual-evaluation.input.embedding_db]


- pipeline:
    name: assistant-pipeline-with-deployment
    nodes:
      - name: embeddings
        type: execution
        step: create-embeddings
      - name: manual-evaluation
        type: execution
        step: do-query
        override:
          parameters:
            - name: question
              default: ["who is Michael Jordan?", "what is a dataset?", "hello", "how to create a model?"]
      - name: deploy
        type: deployment
        deployment: public
        endpoints: [ask]
        actions:
          - when: node-starting
            then: require-approval
    edges:
      - [embeddings.output.*, manual-evaluation.input.embedding_db]
      - [embeddings.output.*, deploy.file.ask.embedding_db]

- pipeline:
    name: rag-evaluation-pipeline
    nodes:
      - name: embeddings
        type: execution
        step: create-embeddings
      - name: generate-responses
        type: execution
        step: do-query
        override:
          parameters:
            - name: question
              default: ["who is Michael Jordan?", "what is a dataset?", "hello", "how to create a model?"]
      - name: evaluate
        type: execution
        step: evaluate-rag
    edges:
      - [embeddings.output.*, generate-responses.input.embedding_db]
      - [generate-responses.output.*, evaluate.input.responses]