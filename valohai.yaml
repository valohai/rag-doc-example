- step:
    name: create-embeddings
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    command: uv run rag-doctor create-database {parameters}
    parameters:
      - name: content_column_index
        description: Index of the document content column in the CSV files
        type: integer
        default: 0
      - name: source_column_index
        description: Index of the source link column in the CSV files
        type: integer
        default: 1
      - name: header_row_skip
        description: Number of initial rows to skip in the CSV files i.e. the header rows
        type: integer
        default: 0
    inputs:
      - name: documentation_csv
        description: CSV file(s) containing the technical documentation
        default: https://valohai-examples.s3.amazonaws.com/rag-doc/huggingface-docs.csv

- step:
    name: single-inference
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    command: uv run rag-doctor query {parameters}
    parameters:
      - name: question
        description: The question to ask about the documentation
        type: string
    inputs:
      - name: embedding_db
        description: Zip archive containing the Qdrant vector database for the embeddings

- endpoint:
    name: ask
    image: ghcr.io/astral-sh/uv:python3.11-bookworm-slim
    port: 8000
    server-command: uv run --group server rag-doctor serve --host 0.0.0.0 --database_dir ./qdrant_data
    files:
      - name: embedding_db
        path: qdrant_data/embeddings.zip
        description: Zip archive containing the Qdrant vector database for the embeddings

- pipeline:
    name: assistant-pipeline
    nodes:
      - name: embeddings
        type: execution
        step: create-embeddings
      - name: smoke-test
        type: execution
        step: single-inference
        override:
          parameters:
            - name: question
              default: "what is a dataset?"
    edges:
      - [embeddings.output.*, smoke-test.input.embedding_db]


- pipeline:
    name: assistant-pipeline-with-deployment
    nodes:
      - name: embeddings
        type: execution
        step: create-embeddings
      - name: smoke-test
        type: execution
        step: single-inference
        override:
          parameters:
            - name: question
              default: "what is a dataset?"
      - name: deploy
        type: deployment
        deployment: public
        endpoints: [ask]
        actions:
          - when: node-starting
            then: require-approval
    edges:
      - [embeddings.output.*, smoke-test.input.embedding_db]
      - [embeddings.output.*, deploy.file.ask.embedding_db]
